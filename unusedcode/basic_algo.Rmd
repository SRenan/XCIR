---
title: "EM"
author: "Renan Sauteraud"
date: "`r date()`"
output: html_document
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(echo = TRUE, message = F)
```


Based on [the wikipedia article for EM](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm).

Let 

- $\theta$ be the parameters of the distributions
- $X$ be the observed data
- $Z$ be the mixture each data point is associated with

#### The EM steps are:

1. Initialize $\theta$
2. E-step: Find the classes $Z$ of each $X$ for the current values of $\theta$
3. M-step: Find the value of $\theta$ for the current values of $Z$
4. Iterate steps 2 and 3 until convergence


#### The quantities we need to derive:

The expected value of the log likelihood of $\theta$ given the current values of $Z|X$ and current estimates of $\theta^{(t)}$.
$$
Q(\theta|\theta^{(t)}) = E_{Z|X, \theta^{(t)}}[\log L(\theta; X,Z)]
$$


Then to find optimal values of the parameters:
$$
\theta^{(t+1)} = argmax Q (\theta|\theta^{(t)})
$$

At step t, $E_{Z|X, \theta^{(t)}}$ is know so we can derive with respect to each
parameters.

$$
\begin{aligned}
\frac{\partial E_{Z|X}(z_i^{(t)})}{\partial \theta_1} &= \sum_{j=1}^m \frac{x_j}{\theta_1} + \frac{n_j-x_j}{1=\theta_1}
\frac{\partial E_{Z|X}(z_i^{(t)})}{\partial \theta_2} &= \sum_{j=1}^m \frac{x_j}{\theta_2} + \frac{n_j-x_j}{1=\theta_2}
\end{aligned}
$$



1. Likelihood
2. Log likelihood


# Mixture of binomial distributions

Derivation based on [stackexchange question](https://stats.stackexchange.com/questions/297776/em-algorithm-for-a-binomial-distribution).

## Derive Q

- Number of genes: m
- Number of reads for gene j: $x_j$
- Number of reads for gene j on least expressed allele: $n_j$


$$
\begin{aligned}
P(X|Z) &= \prod_{i=1}^n P(Xi|Z_i = 1)P(Z_i = 1) + P(Xi|Z_i = 2)P(Z_i = 2)\\
 &= \sum_{j=1}^m (\binom{n_j}{x_j} \theta_1^x(1-\theta_1)^{n_j-x_j} p_1)^{z_i} + 
                 (\binom{n_j}{x_j} \theta_2^x(1-\theta_2)^{n_j-x_j} p_2)^{1-z_i}\\
logL &= \sum_{j=1}^m [z_i](\ln(p_1) + \ln \binom{n_j}{x_j} + x_j \ln(\theta_1) + (n_j-x_j) \ln(1-\theta_1)) +
                     [1-z_i](\ln(p_2) + \ln \binom{n_j}{x_j} + x_j \ln(\theta_2) + (n_j-x_j) \ln(1-\theta_2))\\
\end{aligned}
$$



```{r, eval = F}
sum.finite <- function(x) {
  sum(x[is.finite(x)])
}

while (abs(Q[k]-Q[k-1])>=1e-6){
  comp1 <- p1*dbinom(xj, nj, theta1)
  comp2 <- p2*dbinom(xj, nj, theta2)
  comp_sum <- comp1 + comp2
  p1 <- comp1/comp_sum
  p2 <- comp2/comp_sum
  # M step
  
  p1 <- sum.finite(p1)/length(xj)
  p2 <- sum.finite(p2)/length(xj)
  
  ## ## ## ## ## ## ##

  ## MATH GOES HERE ##

  ## ## ## ## ## ## ##
  
  k <- k + 1
  Q[k] <- sum(log(comp.sum))

}
```

$$
\begin{aligned}
L &= \binom{n}{x} \pi^x(1-\pi)^{n-x}\\
logL &\propto  x\log(\pi) + (n-x)\log(1-\pi)\\
(\theta|\theta^{(t)}) = E_{Z|X, \theta^{(t)}}[\log L(\theta; X,Z)]
\end{aligned}
$$


